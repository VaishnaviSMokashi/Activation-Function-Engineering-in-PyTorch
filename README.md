# Activation-Function-Engineering-in-PyTorch
I modified a ResNet18 architecture by implementing a targeted activation function replacement strategy, swapping ReLU with SiLU activation functions from the 7th occurrence onward while preserving the first 6 layers. This required deep understanding of model architecture and careful implementation.
